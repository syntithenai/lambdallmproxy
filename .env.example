# ================================================================
# Lambda LLM Proxy - Environment Configuration Example
# ================================================================
# Copy this file to .env and fill in your actual values
# ================================================================

# ----------------------------------------------------------------
# DEVELOPMENT SERVER PORTS
# ----------------------------------------------------------------

# Vite development server port (UI)
# Default: 8081
VITE_PORT=8081

# Local Lambda development server port
# Default: 3000
LOCAL_LAMBDA_PORT=3000

# ----------------------------------------------------------------
# AUTHENTICATION & AUTHORIZATION
# ----------------------------------------------------------------

# Comma-separated list of authorized Google account emails
# Previously: ALLOWED_EMAILS
ALLOW_EM=user1@example.com,user2@example.com

# Optional legacy access secret for basic authentication
# Previously: ACCESS_SECRET
ACC_SEC=your-secret-here

# ----------------------------------------------------------------
# GOOGLE OAUTH CONFIGURATION (for YouTube Transcripts)
# ----------------------------------------------------------------

# Google OAuth 2.0 credentials for YouTube Data API v3
# Get these from: https://console.cloud.google.com/apis/credentials
# Previously: GOOGLE_CLIENT_ID
GGL_CID=your-client-id.apps.googleusercontent.com
# Previously: GOOGLE_CLIENT_SECRET
GGL_SEC=GOCSPX-your-client-secret
# Previously: OAUTH_REDIRECT_URI
OAUTH_URI=https://your-lambda-url.lambda-url.us-east-1.on.aws/oauth/callback

# ----------------------------------------------------------------
# IMAGE SEARCH APIS (for Feed Feature)
# ----------------------------------------------------------------

# Unsplash API (Primary image provider - free tier: 50 requests/hour)
# Get API key from: https://unsplash.com/developers
# Required for feed image search feature
UNSPLASH_ACCESS_KEY=your-unsplash-access-key-here

# Pexels API (Fallback image provider - free tier: 200 requests/hour)
# Get API key from: https://www.pexels.com/api/
# Used as fallback when Unsplash has no results
PEXELS_API_KEY=your-pexels-api-key-here

# Webshare proxy configuration (for YouTube transcript fetching from AWS)
# Required because YouTube blocks AWS/cloud provider IPs
# Sign up at: https://www.webshare.io (purchase "Residential" proxies)
# Get credentials from: https://dashboard.webshare.io/proxy/settings
# Use the "Proxy Username" and "Proxy Password" values
# Previously: WEBSHARE_PROXY_USERNAME
PXY_USER=your-proxy-username
# Previously: WEBSHARE_PROXY_PASSWORD
PXY_PASS=your-proxy-password

# ----------------------------------------------------------------
# PROVIDER CONFIGURATION (New Format - REQUIRED)
# ----------------------------------------------------------------
# Configure multiple API providers using indexed environment variables
# Format: P_<FIELD><INDEX> (compressed from LLAMDA_LLM_PROXY_PROVIDER_<FIELD>_<INDEX>)
#
# Required fields per provider:
#   P_T<N>   - Provider type (openai, groq-free, gemini-free, openai-compatible)
#   P_K<N>   - API key for the provider
#
# Optional fields:
#   P_E<N>   - Custom API endpoint (for openai-compatible providers)
#   P_M<N>   - Default model name (for openai-compatible providers)
#   P_RL<N>  - Rate limit in tokens per minute (for openai-compatible)
#   P_AM<N>  - Comma-separated list of allowed models (empty=all, non-empty=exact match)
#   P_IQ<N>  - Max image quality: fast|standard|high|ultra
#   P_P<N>   - Selection priority (lower number = higher priority, default=100)
#                          Priority 1 = highest priority (selected first)
#                          Priority 999 = lowest priority (selected last)
#                          Used across ALL endpoints (chat, embeddings, transcription, image generation)
#
# These providers are ONLY available to authorized users (in ALLOWED_EMAILS)
#
# Get API keys from:
#   - OpenAI: https://platform.openai.com/api-keys
#   - Groq: https://console.groq.com/keys
#   - Gemini: https://makersuite.google.com/app/apikey
# ----------------------------------------------------------------

# Provider 0: Groq Free Tier (example)
P_T0=groq-free
P_K0=gsk_your-groq-key-here
P_P0=10
# Optional: Restrict to specific models (leave empty to allow all)
# Previously: LLAMDA_LLM_PROXY_PROVIDER_ALLOWED_MODELS_0=llama-3.1-8b-instant,llama-3.3-70b-versatile

# Provider 1: OpenAI (example)
P_T1=openai
P_K1=sk-proj-your-openai-key-here
P_P1=1
# Optional: Allow all models
# Previously: LLAMDA_LLM_PROXY_PROVIDER_ALLOWED_MODELS_1=

# Provider 2: Gemini Free Tier (example)
P_T2=gemini-free
P_K2=AIza-your-gemini-key-here
P_P2=50

# Provider 3: Together AI - Restricted to FREE image generation only
# Previously: LLAMDA_LLM_PROXY_PROVIDER_TYPE_3=together
# Previously: LLAMDA_LLM_PROXY_PROVIDER_KEY_3=your-together-api-key
# Previously: LLAMDA_LLM_PROXY_PROVIDER_PRIORITY_3=100
# Previously: LLAMDA_LLM_PROXY_PROVIDER_ALLOWED_MODELS_3=black-forest-labs/FLUX.1-schnell-Free
# Previously: LLAMDA_LLM_PROXY_PROVIDER_IMAGE_MAX_QUALITY_3=fast

# Provider 4: LM Studio Local (FREE, runs locally - example - optional)
# Run LM Studio on your machine and enable local server (default port: 1234)
# Download from: https://lmstudio.ai/
# No API key required for local access
# P_T4=openai-compatible
# P_K4=lm-studio
# P_E4=http://localhost:1234/v1
# P_M4=local-model
# P_P4=50
# Note: Replace 'local-model' with the actual model name loaded in LM Studio
# Note: If accessing from Lambda (AWS), use your machine's external IP instead of localhost

# Provider 5: Ollama Local (FREE, runs locally - example - optional)
# Run Ollama on your machine (default port: 11434)
# Download from: https://ollama.ai/
# No API key required for local access
# P_T5=openai-compatible
# P_K5=ollama
# P_E5=http://localhost:11434/v1
# P_M5=llama3.2
# P_P5=50
# Note: Replace 'llama3.2' with the model name you've pulled in Ollama
# Note: If accessing from Lambda (AWS), use your machine's external IP instead of localhost

# Provider 6: Custom OpenAI-compatible endpoint (example - optional)
# Previously: LLAMDA_LLM_PROXY_PROVIDER_TYPE_6=openai-compatible
# Previously: LLAMDA_LLM_PROXY_PROVIDER_KEY_6=your-api-key
# Previously: LLAMDA_LLM_PROXY_PROVIDER_PRIORITY_6=200
# Previously: LLAMDA_LLM_PROXY_PROVIDER_ENDPOINT_6=https://api.your-provider.com/v1
# Previously: LLAMDA_LLM_PROXY_PROVIDER_MODEL_6=your-model-name
# Previously: LLAMDA_LLM_PROXY_PROVIDER_RATE_LIMIT_6=100000

# Provider 7: Speaches Local TTS/STT (FREE, self-hosted - example - optional)
# Requires Docker container running: docker-compose -f docker-compose.speaches.yml up -d
# P_T7=speaches
# P_K7=dummy-key
# P_E7=http://localhost:8000
# P_C7=voice,tts
# P_P7=5
# Note: Speaches accepts any key when running locally without authentication
# For production deployment with custom endpoint: P_E7=https://speaches.yourdomain.com

# Provider 8: Anthropic Claude (PREMIUM - example - optional)
# Get API key from: https://console.anthropic.com/
# Supports: claude-sonnet-4-5, claude-3-7-sonnet, claude-3-5-haiku
# Pricing: Flagship models $3/$15 per M tokens, Haiku $0.80/$4 per M tokens
# All models support: 200K context, tools, streaming
# P_T8=anthropic
# P_K8=sk-ant-api03-your-anthropic-key-here
# P_P8=1
# Optional: Allow all Claude models (leave empty for all)
# P_AM8=

# ----------------------------------------------------------------
# TOOL CONFIGURATION
# ----------------------------------------------------------------

# Maximum number of tool call iterations before stopping (default: 15)
MAX_ITER=10

# Disable YouTube Whisper transcription (does NOT affect YouTube API transcripts via OAuth)
# Set to 'true' to disable using Whisper for YouTube video transcription
# When true: YouTube videos can ONLY be transcribed via YouTube API (requires OAuth)
# When false: YouTube videos can use BOTH Whisper AND YouTube API transcription
# NOTE: YouTube SEARCH always works regardless of this setting
# Other media types (direct audio/video URLs) always use Whisper regardless of this setting
NO_YT_TRANS=false

# Media download timeout in milliseconds (default: 30000 = 30 seconds)
MED_TIMEOUT=30000

# ----------------------------------------------------------------
# CONTENT MODERATION / GUARDRAILS
# ----------------------------------------------------------------

# Enable content guardrails (input and output filtering)
# When enabled, ALL requests are filtered before processing
# and ALL responses are filtered before returning to user
# Provider and model selection is AUTOMATIC based on available API keys
# Preference order: groq-free > gemini-free > groq > other providers
# Automatically selects dedicated guardrail models when available (e.g., Llama Guard 4)
# Default: false (disabled)
EN_GUARD=false

# NOTE: No additional configuration needed!
# The system automatically detects the best available provider and model
# from your configured providers (LLAMDA_LLM_PROXY_PROVIDER_* variables)
# or UI-provided API keys.
#
# For example, if you have groq-free configured (Provider 0 above), 
# the system will automatically use meta-llama/llama-guard-4-12b for content filtering.

# ----------------------------------------------------------------
# WEB SCRAPING CONFIGURATION
# ----------------------------------------------------------------

# Puppeteer Lambda function ARN (for JavaScript-rendered pages)
# When set: Direct scraping tries first, falls back to Puppeteer Lambda if needed
# When not set: Only direct scraping is used (Tavily or DuckDuckGo with proxy)
# 
# Setup instructions:
#   1. Run: ./scripts/setup-puppeteer-function.sh
#   2. Run: ./scripts/deploy-puppeteer-lambda.sh
#   3. Run: ./scripts/setup-main-lambda-permissions.sh
#   4. Copy the ARN from the output and set it here
# 
# Example: PUPPETEER_LAMBDA_ARN=arn:aws:lambda:us-east-1:123456789012:function:llmproxy-puppeteer
PPT_ARN=

# Enable/disable Puppeteer fallback (default: true)
# When true: Falls back to Puppeteer Lambda if direct scraping fails
# When false: Only direct scraping is attempted
USE_PPT=true

# YouTube Data API v3 Key (for YouTube search tool)
YOUTUBE_API_KEY=your-youtube-api-key-here

# ----------------------------------------------------------------
# SHARE LINK CONFIGURATION
# ----------------------------------------------------------------
# Base URL for shared conversations and snippets
# Production: http://ai.syntithenai.com
# Local dev: http://localhost:8081 or http://localhost:5173
SHARE_BASE_URL=http://ai.syntithenai.com

# ----------------------------------------------------------------
# MODEL CONFIGURATION
# ----------------------------------------------------------------

# Default Groq model for tool execution fallback
GROQ_MDL=llama-3.1-8b-instant

# Default OpenAI model for tool execution fallback
# Previously: OPENAI_MODEL=gpt-4o-mini

# Comma-separated list of Groq models that support reasoning
# Previously: GROQ_REASONING_MODELS=llama-3.3-70b-versatile

# Reasoning effort level for reasoning models: low, medium, or high
REASON_EFF=medium

# Custom OpenAI API base URL (for self-hosted or proxy endpoints)
# Previously: OPENAI_API_BASE=https://api.openai.com

# ----------------------------------------------------------------
# SYSTEM PROMPTS & TEMPLATES (Optional Overrides)
# ----------------------------------------------------------------
# These have sensible defaults in the code. Only set if you need custom behavior.

# Main system prompt for search-enabled queries
# Previously: SYSTEM_PROMPT_SEARCH=You are a highly knowledgeable AI assistant with access to powerful research and computational tools...

# System prompt for search result analysis
# Previously: SYSTEM_PROMPT_DIGEST_ANALYST=You are a thorough research analyst that extracts comprehensive information...

# Final response template for multi-search synthesis
# Previously: FINAL_TEMPLATE=Based on comprehensive multi-search research, provide the most complete answer...

# ----------------------------------------------------------------
# IMAGE GENERATION PROVIDERS (Optional)
# ----------------------------------------------------------------
# Configure API keys for image generation providers
# At least one provider must be configured for image generation to work

# OpenAI DALL-E (DALL-E 2 and DALL-E 3)
# Get API key from: https://platform.openai.com/api-keys
# Pricing: DALL-E 3: $0.040-0.120/image, DALL-E 2: $0.016-0.020/image
# Previously: OPENAI_API_KEY
OPENAI_KEY=sk-your-openai-api-key-here

# Together AI (Stable Diffusion models)
# Get API key from: https://api.together.xyz/settings/api-keys
# Pricing: $0.001-0.003/image (more affordable than DALL-E)
# Previously: TOGETHER_API_KEY
TOGETHER_KEY=your-together-api-key-here

# Replicate (Various Stable Diffusion models)
# Get API token from: https://replicate.com/account/api-tokens
# Pricing: ~$0.0018-0.0025/image (pay-per-use, billed by compute time)
# Previously: REPLICATE_API_TOKEN
REPLICATE_KEY=r8_your-replicate-token-here

# Google Gemini/Imagen (Not yet available)
# Note: Google has not released public API for image generation
# This is reserved for future compatibility
# Previously: GEMINI_API_KEY
GEMINI_KEY=your-gemini-api-key-here

# ----------------------------------------------------------------
# IMAGE GENERATION FEATURE FLAGS (Optional)
# ----------------------------------------------------------------
# Enable/disable specific providers for image generation
# Default: true if API key exists, false otherwise

IMG_OPENAI=true
IMG_TOGETHER=true
IMG_REPLICATE=true
IMG_GEMINI=false

# ----------------------------------------------------------------
# VOICE MODE / CONTINUOUS CONVERSATION
# ----------------------------------------------------------------
# NOTE: Continuous voice mode now uses FREE Browser Speech Recognition API
# No API key needed! Works in Chrome, Edge, and Safari.
# 
# Trade-offs vs Picovoice Porcupine:
# - FREE (no cost, no API key)
# - No setup required
# - Works with any custom wake word
# - Requires internet connection (uses cloud speech recognition)
# - Less accurate than Porcupine (~70-80% vs 95%+)
# - Higher latency (~500ms vs <200ms)
# 
# Wake words are configured in the UI settings panel
# Examples: "hey google", "jarvis", "alexa", "computer", or any custom phrase
#
# No configuration needed - just works!

# ----------------------------------------------------------------
# IMAGE GENERATION CIRCUIT BREAKER (Optional)
# ----------------------------------------------------------------
# Circuit breaker prevents repeated calls to failing providers
# After FAILURE_THRESHOLD failures in TIMEOUT_MS, provider is disabled
# Provider automatically retries after TIMEOUT_MS (half-open state)

# Number of failures before opening circuit (default: 5)
CB_THRESH=5

# Time window for failure tracking in milliseconds (default: 600000 = 10 minutes)
CB_TIMEOUT=600000

# ----------------------------------------------------------------
# GOOGLE SHEETS LOGGING (Optional)
# ----------------------------------------------------------------
# Enable automatic logging of LLM API requests to Google Sheets
# See GOOGLE_SHEETS_LOGGING_SETUP.md for detailed setup instructions
# Leave these unset to disable logging
#
# NOTE: Error reporting (error-reporter.js) automatically uses the first
# spreadsheet from GS_SHEET_IDS, so no separate config needed.

# Your Google Sheet ID(s) - comma-separated for multiple sheets (sharding)
# Example: https://docs.google.com/spreadsheets/d/1a2b3c4d5e6f7g8h9i0/edit
#                                                   ^^^^^^^^^^^^^^^^^^^
# Use GS_SHEET_IDS for multiple sheets or GS_SHEET_ID for single sheet
# Previously: GOOGLE_SHEETS_LOG_SPREADSHEET_IDS
GS_SHEET_IDS=1a2b3c4d5e6f7g8h9i0

# Service account email (from the JSON key file)
# Previously: GOOGLE_SHEETS_SERVICE_ACCOUNT_EMAIL
GS_EMAIL=llm-logger@your-project.iam.gserviceaccount.com

# Service account private key (from the JSON key file)
# IMPORTANT: Must include full key with -----BEGIN/END PRIVATE KEY----- markers
# Use \\n for newlines (double backslash), wrap entire key in quotes
# Previously: GOOGLE_SHEETS_SERVICE_ACCOUNT_PRIVATE_KEY
GS_KEY="-----BEGIN PRIVATE KEY-----\\nMIIEvQIBADANBg...full_key_here...\\n-----END PRIVATE KEY-----\\n"

# Optional: Sheet name (defaults to "LLM Usage Log")
# Previously: GOOGLE_SHEETS_LOG_SHEET_NAME
GS_NAME=LLM Usage Log

# ----------------------------------------------------------------
# AWS LAMBDA (Auto-set by AWS - do not configure manually)
# ----------------------------------------------------------------
# Previously: AWS_LAMBDA_FUNCTION_MEMORY_SIZE - Auto-set by Lambda runtime

# ================================================================
# DEPLOYMENT INSTRUCTIONS
# ================================================================
#
# 1. Copy this file:
#    cp .env.example .env
#
# 2. Fill in your actual API keys and configuration
#
# 3. Deploy to Lambda:
#    make deploy-env
#
# 4. Verify deployment:
#    make logs
#
# ================================================================
