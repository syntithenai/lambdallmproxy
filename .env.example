# ================================================================
# Lambda LLM Proxy - Environment Configuration Example
# ================================================================
# Copy this file to .env and fill in your actual values
# ================================================================

# ----------------------------------------------------------------
# AUTHENTICATION & AUTHORIZATION
# ----------------------------------------------------------------

# Comma-separated list of authorized Google account emails
# Previously: ALLOWED_EMAILS
ALLOW_EM=user1@example.com,user2@example.com

# Optional legacy access secret for basic authentication
# Previously: ACCESS_SECRET
ACC_SEC=your-secret-here

# ----------------------------------------------------------------
# GOOGLE OAUTH CONFIGURATION (for YouTube Transcripts)
# ----------------------------------------------------------------

# Google OAuth 2.0 credentials for YouTube Data API v3
# Get these from: https://console.cloud.google.com/apis/credentials
# Previously: GOOGLE_CLIENT_ID
GGL_CID=your-client-id.apps.googleusercontent.com
# Previously: GOOGLE_CLIENT_SECRET
GGL_SEC=GOCSPX-your-client-secret
# Previously: OAUTH_REDIRECT_URI
OAUTH_URI=https://your-lambda-url.lambda-url.us-east-1.on.aws/oauth/callback

# Webshare proxy configuration (for YouTube transcript fetching from AWS)
# Required because YouTube blocks AWS/cloud provider IPs
# Sign up at: https://www.webshare.io (purchase "Residential" proxies)
# Get credentials from: https://dashboard.webshare.io/proxy/settings
# Use the "Proxy Username" and "Proxy Password" values
# Previously: WEBSHARE_PROXY_USERNAME
PXY_USER=your-proxy-username
# Previously: WEBSHARE_PROXY_PASSWORD
PXY_PASS=your-proxy-password

# ----------------------------------------------------------------
# PROVIDER CONFIGURATION (New Format - REQUIRED)
# ----------------------------------------------------------------
# Configure multiple API providers using indexed environment variables
# Format: P_<FIELD><INDEX> (compressed from LLAMDA_LLM_PROXY_PROVIDER_<FIELD>_<INDEX>)
#
# Required fields per provider:
#   P_T<N>   - Provider type (openai, groq-free, gemini-free, openai-compatible)
#   P_K<N>   - API key for the provider
#
# Optional fields:
#   P_E<N>   - Custom API endpoint (for openai-compatible providers)
#   P_M<N>   - Default model name (for openai-compatible providers)
#   P_RL<N>  - Rate limit in tokens per minute (for openai-compatible)
#   P_AM<N>  - Comma-separated list of allowed models (empty=all, non-empty=exact match)
#   P_IQ<N>  - Max image quality: fast|standard|high|ultra
#   P_P<N>   - Selection priority (lower number = higher priority, default=100)
#                          Priority 1 = highest priority (selected first)
#                          Priority 999 = lowest priority (selected last)
#                          Used across ALL endpoints (chat, embeddings, transcription, image generation)
#
# These providers are ONLY available to authorized users (in ALLOWED_EMAILS)
#
# Get API keys from:
#   - OpenAI: https://platform.openai.com/api-keys
#   - Groq: https://console.groq.com/keys
#   - Gemini: https://makersuite.google.com/app/apikey
# ----------------------------------------------------------------

# Provider 0: Groq Free Tier (example)
P_T0=groq-free
P_K0=gsk_your-groq-key-here
P_P0=10
# Optional: Restrict to specific models (leave empty to allow all)
# Previously: LLAMDA_LLM_PROXY_PROVIDER_ALLOWED_MODELS_0=llama-3.1-8b-instant,llama-3.3-70b-versatile

# Provider 1: OpenAI (example)
P_T1=openai
P_K1=sk-proj-your-openai-key-here
P_P1=1
# Optional: Allow all models
# Previously: LLAMDA_LLM_PROXY_PROVIDER_ALLOWED_MODELS_1=

# Provider 2: Gemini Free Tier (example)
P_T2=gemini-free
P_K2=AIza-your-gemini-key-here
P_P2=50

# Provider 3: Together AI - Restricted to FREE image generation only
# Previously: LLAMDA_LLM_PROXY_PROVIDER_TYPE_3=together
# Previously: LLAMDA_LLM_PROXY_PROVIDER_KEY_3=your-together-api-key
# Previously: LLAMDA_LLM_PROXY_PROVIDER_PRIORITY_3=100
# Previously: LLAMDA_LLM_PROXY_PROVIDER_ALLOWED_MODELS_3=black-forest-labs/FLUX.1-schnell-Free
# Previously: LLAMDA_LLM_PROXY_PROVIDER_IMAGE_MAX_QUALITY_3=fast

# Provider 4: Custom OpenAI-compatible endpoint (example - optional)
# Previously: LLAMDA_LLM_PROXY_PROVIDER_TYPE_4=openai-compatible
# Previously: LLAMDA_LLM_PROXY_PROVIDER_KEY_4=your-api-key
# Previously: LLAMDA_LLM_PROXY_PROVIDER_PRIORITY_4=200
# Previously: LLAMDA_LLM_PROXY_PROVIDER_ENDPOINT_4=https://api.your-provider.com/v1
# Previously: LLAMDA_LLM_PROXY_PROVIDER_MODEL_4=your-model-name
# Previously: LLAMDA_LLM_PROXY_PROVIDER_RATE_LIMIT_4=100000

# ----------------------------------------------------------------
# TOOL CONFIGURATION
# ----------------------------------------------------------------

# Maximum number of tool call iterations before stopping (default: 15)
MAX_ITER=10

# Disable YouTube Whisper transcription (does NOT affect YouTube API transcripts via OAuth)
# Set to 'true' to disable using Whisper for YouTube video transcription
# When true: YouTube videos can ONLY be transcribed via YouTube API (requires OAuth)
# When false: YouTube videos can use BOTH Whisper AND YouTube API transcription
# NOTE: YouTube SEARCH always works regardless of this setting
# Other media types (direct audio/video URLs) always use Whisper regardless of this setting
NO_YT_TRANS=false

# Media download timeout in milliseconds (default: 30000 = 30 seconds)
MED_TIMEOUT=30000

# ----------------------------------------------------------------
# CONTENT MODERATION / GUARDRAILS
# ----------------------------------------------------------------

# Enable content guardrails (input and output filtering)
# When enabled, ALL requests are filtered before processing
# and ALL responses are filtered before returning to user
# Provider and model selection is AUTOMATIC based on available API keys
# Preference order: groq-free > gemini-free > groq > other providers
# Automatically selects dedicated guardrail models when available (e.g., Llama Guard 4)
# Default: false (disabled)
EN_GUARD=false

# NOTE: No additional configuration needed!
# The system automatically detects the best available provider and model
# from your configured providers (LLAMDA_LLM_PROXY_PROVIDER_* variables)
# or UI-provided API keys.
#
# For example, if you have groq-free configured (Provider 0 above), 
# the system will automatically use meta-llama/llama-guard-4-12b for content filtering.

# ----------------------------------------------------------------
# WEB SCRAPING CONFIGURATION
# ----------------------------------------------------------------

# Puppeteer Lambda function ARN (for JavaScript-rendered pages)
# When set: Direct scraping tries first, falls back to Puppeteer Lambda if needed
# When not set: Only direct scraping is used (Tavily or DuckDuckGo with proxy)
# 
# Setup instructions:
#   1. Run: ./scripts/setup-puppeteer-function.sh
#   2. Run: ./scripts/deploy-puppeteer-lambda.sh
#   3. Run: ./scripts/setup-main-lambda-permissions.sh
#   4. Copy the ARN from the output and set it here
# 
# Example: PUPPETEER_LAMBDA_ARN=arn:aws:lambda:us-east-1:123456789012:function:llmproxy-puppeteer
PPT_ARN=

# Enable/disable Puppeteer fallback (default: true)
# When true: Falls back to Puppeteer Lambda if direct scraping fails
# When false: Only direct scraping is attempted
USE_PPT=true

# ----------------------------------------------------------------
# MODEL CONFIGURATION
# ----------------------------------------------------------------

# Default Groq model for tool execution fallback
GROQ_MDL=llama-3.1-8b-instant

# Default OpenAI model for tool execution fallback
# Previously: OPENAI_MODEL=gpt-4o-mini

# Comma-separated list of Groq models that support reasoning
# Previously: GROQ_REASONING_MODELS=llama-3.3-70b-versatile

# Reasoning effort level for reasoning models: low, medium, or high
REASON_EFF=medium

# Custom OpenAI API base URL (for self-hosted or proxy endpoints)
# Previously: OPENAI_API_BASE=https://api.openai.com

# ----------------------------------------------------------------
# SYSTEM PROMPTS & TEMPLATES (Optional Overrides)
# ----------------------------------------------------------------
# These have sensible defaults in the code. Only set if you need custom behavior.

# Main system prompt for search-enabled queries
# Previously: SYSTEM_PROMPT_SEARCH=You are a highly knowledgeable AI assistant with access to powerful research and computational tools...

# System prompt for search result analysis
# Previously: SYSTEM_PROMPT_DIGEST_ANALYST=You are a thorough research analyst that extracts comprehensive information...

# Final response template for multi-search synthesis
# Previously: FINAL_TEMPLATE=Based on comprehensive multi-search research, provide the most complete answer...

# ----------------------------------------------------------------
# IMAGE GENERATION PROVIDERS (Optional)
# ----------------------------------------------------------------
# Configure API keys for image generation providers
# At least one provider must be configured for image generation to work

# OpenAI DALL-E (DALL-E 2 and DALL-E 3)
# Get API key from: https://platform.openai.com/api-keys
# Pricing: DALL-E 3: $0.040-0.120/image, DALL-E 2: $0.016-0.020/image
# Previously: OPENAI_API_KEY
OPENAI_KEY=sk-your-openai-api-key-here

# Together AI (Stable Diffusion models)
# Get API key from: https://api.together.xyz/settings/api-keys
# Pricing: $0.001-0.003/image (more affordable than DALL-E)
# Previously: TOGETHER_API_KEY
TOGETHER_KEY=your-together-api-key-here

# Replicate (Various Stable Diffusion models)
# Get API token from: https://replicate.com/account/api-tokens
# Pricing: ~$0.0018-0.0025/image (pay-per-use, billed by compute time)
# Previously: REPLICATE_API_TOKEN
REPLICATE_KEY=r8_your-replicate-token-here

# Google Gemini/Imagen (Not yet available)
# Note: Google has not released public API for image generation
# This is reserved for future compatibility
# Previously: GEMINI_API_KEY
GEMINI_KEY=your-gemini-api-key-here

# ----------------------------------------------------------------
# IMAGE GENERATION FEATURE FLAGS (Optional)
# ----------------------------------------------------------------
# Enable/disable specific providers for image generation
# Default: true if API key exists, false otherwise

IMG_OPENAI=true
IMG_TOGETHER=true
IMG_REPLICATE=true
IMG_GEMINI=false

# ----------------------------------------------------------------
# IMAGE GENERATION CIRCUIT BREAKER (Optional)
# ----------------------------------------------------------------
# Circuit breaker prevents repeated calls to failing providers
# After FAILURE_THRESHOLD failures in TIMEOUT_MS, provider is disabled
# Provider automatically retries after TIMEOUT_MS (half-open state)

# Number of failures before opening circuit (default: 5)
CB_THRESH=5

# Time window for failure tracking in milliseconds (default: 600000 = 10 minutes)
CB_TIMEOUT=600000

# ----------------------------------------------------------------
# GOOGLE SHEETS LOGGING (Optional)
# ----------------------------------------------------------------
# Enable automatic logging of LLM API requests to Google Sheets
# See GOOGLE_SHEETS_LOGGING_SETUP.md for detailed setup instructions
# Leave these unset to disable logging

# Your Google Sheet ID (from the spreadsheet URL)
# Example: https://docs.google.com/spreadsheets/d/1a2b3c4d5e6f7g8h9i0/edit
#                                                   ^^^^^^^^^^^^^^^^^^^
# Previously: GOOGLE_SHEETS_LOG_SPREADSHEET_ID
GS_SHEET_ID=1a2b3c4d5e6f7g8h9i0

# Service account email (from the JSON key file)
# Previously: GOOGLE_SHEETS_SERVICE_ACCOUNT_EMAIL
GS_EMAIL=llm-logger@your-project.iam.gserviceaccount.com

# Service account private key (from the JSON key file)
# IMPORTANT: Must include full key with -----BEGIN/END PRIVATE KEY----- markers
# Use \\n for newlines (double backslash), wrap entire key in quotes
# Previously: GOOGLE_SHEETS_SERVICE_ACCOUNT_PRIVATE_KEY
GS_KEY="-----BEGIN PRIVATE KEY-----\\nMIIEvQIBADANBg...full_key_here...\\n-----END PRIVATE KEY-----\\n"

# Optional: Sheet name (defaults to "LLM Usage Log")
# Previously: GOOGLE_SHEETS_LOG_SHEET_NAME
GS_NAME=LLM Usage Log

# ----------------------------------------------------------------
# AWS LAMBDA (Auto-set by AWS - do not configure manually)
# ----------------------------------------------------------------
# Previously: AWS_LAMBDA_FUNCTION_MEMORY_SIZE - Auto-set by Lambda runtime

# ================================================================
# DEPLOYMENT INSTRUCTIONS
# ================================================================
#
# 1. Copy this file:
#    cp .env.example .env
#
# 2. Fill in your actual API keys and configuration
#
# 3. Deploy to Lambda:
#    make deploy-env
#
# 4. Verify deployment:
#    make logs
#
# ================================================================
