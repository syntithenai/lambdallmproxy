# Web Scraping Content Preservation - Deployment Guide

**Implementation Complete:** October 17, 2025  
**Status:** ‚úÖ Ready for Production Deployment  
**Testing Status:** All unit and integration tests passed

---

## What Was Implemented

### Phase 2: Image Placement Intelligence ‚úÖ
**Files Modified:** `src/html-parser.js`, `src/endpoints/chat.js`

**Features:**
1. Added `classifyImagePlacement()` method that detects:
   - Hero images (score: 1.0) - Large images in top 10% of page
   - Above-fold (score: 0.9) - Images in top 20% of page
   - Sidebar (score: 0.3) - Images in sidebar/aside/widget containers
   - Content (score: 0.8) - Images in article/main containers
   - Below-fold (score: 0.5) - Default for other images

2. Modified `extractImages()` to add placement metadata to each image:
   ```javascript
   {
     src: "...",
     alt: "...",
     placement: "hero",           // NEW
     placementScore: 1.0,         // NEW
     relevance: 0.6,
     width: 1200,
     height: 600
   }
   ```

3. Smart image selection in chat.js (lines 2405-2430):
   ```javascript
   // Weighted scoring: 60% placement + 40% relevance
   const scoreA = (a.placementScore || 0.5) * 0.6 + (a.relevance || 0.5) * 0.4;
   ```
   - Prioritizes hero images over sidebar ads
   - Ranks content images above footer/navigation
   - Top 3 images selected for UI display

### Phase 3: Intelligent Content Truncation ‚úÖ
**Files Modified:** `src/tools.js`

**Changes:**
- Replaced `substring(0, 300)` with `extractKeyContent()` calls
- Lines modified: 1294, 1295
- Preserves:
  - Query-relevant sentences
  - Numerical data (percentages, metrics, counts)
  - Dates and temporal information
  - Headers and important markers
  - Contextual information

**Before:**
```javascript
content: r.content ? r.content.substring(0, 300) : r.content
```

**After:**
```javascript
content: r.content ? extractKeyContent(r.content, r.query) : r.content
```

### Phase 4: Transcript Dual-Path Delivery ‚úÖ
**Files Modified:** `src/tools.js`, `src/endpoints/chat.js`

**Features:**
1. **UI receives full transcript** (chat.js lines 2358-2375):
   ```javascript
   extractedContent.transcripts.push({
     videoId, videoUrl, title,
     fullTranscript,    // Complete text
     segments,          // Timestamped segments
     duration, thumbnail, chapters
   });
   ```

2. **LLM receives compressed summary** (tools.js lines 2712-2970):
   ```javascript
   // Model-aware compression
   function summarizeTranscriptForLLM(transcript, model) {
     const contextWindow = model?.context_window || 32000;
     let maxChars = contextWindow > 100000 ? 2000 : 
                    contextWindow > 16000 ? 1000 : 400;
     return extractKeyContent(fullText, null, maxChars);
   }
   
   // Extract important sentences
   function extractKeyQuotes(transcript, count = 5) {
     // Returns top 5 most significant sentences
   }
   
   // Dual-path return structure
   return JSON.stringify({
     // For UI
     transcript: fullText,
     segments: result.snippets,
     // For LLM
     llmSummary: summarizeTranscriptForLLM(fullText, model),
     keyQuotes: extractKeyQuotes(fullText),
     // Metadata
     videoId, videoUrl, title, duration...
   });
   ```

---

## Testing Results

### Unit Tests ‚úÖ
**File:** `test-web-scraping.js`

- ‚úÖ Content truncation preserves important info (100%)
- ‚úÖ Query-relevant content prioritized
- ‚úÖ Numerical data preserved
- ‚úÖ Date information retained
- ‚úÖ Headers and markers preserved

### Integration Tests ‚úÖ
**File:** `test-web-scraping-integration.js`

**Test 1: News Article**
```
Hero image: placement=hero, score=1.0
Content images: placement=below-fold, score=0.5
Top 3 selection: Hero ranked #1, content images #2-3
```

**Test 2: Blog Post**
```
Hero image: placement=above-fold, score=0.9
Product images: placement=below-fold, score=0.5
Sidebar ads: placement=sidebar, score=0.3
Top 3 selection: Hero, products prioritized over ads
```

**Validation:**
- ‚úÖ No syntax errors in modified files
- ‚úÖ Image classification working correctly
- ‚úÖ Smart selection prioritizes high-value images
- ‚úÖ Content extraction preserves key information

**Full Results:** See `TESTING_RESULTS.md`

---

## Deployment Steps

### Step 1: Pre-Deployment Checklist

```bash
# Verify no syntax errors
cd /home/stever/projects/lambdallmproxy
node -c src/html-parser.js
node -c src/tools.js  
node -c src/endpoints/chat.js

# Run tests one more time
node test-web-scraping.js
node test-web-scraping-integration.js

# Check git status
git status
```

### Step 2: Commit Changes

```bash
# Stage modified files
git add src/html-parser.js src/tools.js src/endpoints/chat.js

# Stage documentation
git add IMPLEMENTATION_SUMMARY.md TESTING_RESULTS.md WEB_SCRAPING_CONTENT_PRESERVATION_PLAN.md

# Stage test files
git add test-web-scraping.js test-web-scraping-integration.js

# Commit
git commit -m "feat: web scraping content preservation - image placement, smart selection, transcript dual-path

- Add image placement classification (hero/above-fold/content/sidebar/below-fold)
- Implement smart selection with weighted scoring (60% placement + 40% relevance)
- Replace substring truncation with intelligent extractKeyContent()
- Add transcript dual-path delivery (full to UI, summary to LLM)
- Add model-aware compression for transcripts
- All unit and integration tests passing
"
```

### Step 3: Deploy to Lambda

```bash
# Deploy function
./deploy.sh

# Monitor deployment
tail -f deploy.log
```

### Step 4: Verify Deployment

```bash
# Check function is updated
aws lambda get-function --function-name YOUR_FUNCTION_NAME --query 'Configuration.LastModified'

# Test basic functionality
curl -X POST https://YOUR_LAMBDA_URL/health
```

---

## Production Testing Checklist

### Test 1: Web Search with Images üì∏

**Test Query:**
```bash
curl -X POST https://YOUR_LAMBDA_URL/chat \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Search for latest AI breakthrough news"}
    ],
    "tools": [{"name": "web_search"}],
    "model": "gpt-4"
  }'
```

**Validate:**
- [ ] Response includes `extractedContent.images` array
- [ ] Images have `placement` and `placementScore` fields
- [ ] Top 3 images prioritized by combined score
- [ ] Hero images appear before sidebar/footer images
- [ ] Check CloudWatch logs for "classifyImagePlacement" messages

**Expected Output Structure:**
```json
{
  "extractedContent": {
    "images": [
      {
        "src": "https://example.com/hero.jpg",
        "placement": "hero",
        "placementScore": 1.0,
        "relevance": 0.6,
        "llmContext": {
          "placement": "hero",
          "suggestedPosition": "above-content"
        }
      }
    ]
  }
}
```

### Test 2: Direct URL Scraping ‚úÇÔ∏è

**Test URL:**
```bash
curl -X POST https://YOUR_LAMBDA_URL/chat \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Scrape this article: https://techcrunch.com/SOME_ARTICLE"}
    ],
    "tools": [{"name": "scrape_url"}],
    "model": "gpt-4"
  }'
```

**Validate:**
- [ ] Content truncated using `extractKeyContent` (not substring)
- [ ] Important data preserved (numbers, dates, key phrases)
- [ ] Descriptions <= 300 chars but informative
- [ ] Check logs for "Intelligent truncation" or "Smart extraction"

### Test 3: YouTube Short Video (~5 min) üìπ

**Test Video:**
```bash
curl -X POST https://YOUR_LAMBDA_URL/chat \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Get transcript: https://youtube.com/watch?v=SHORT_VIDEO_ID"}
    ],
    "tools": [{"name": "get_youtube_transcript"}],
    "model": "gpt-4"
  }'
```

**Validate:**
- [ ] `extractedContent.transcripts` contains full transcript
- [ ] Response includes `llmSummary` (compressed version)
- [ ] Response includes `keyQuotes` array (3-5 items)
- [ ] Summary significantly shorter than full transcript
- [ ] Check logs for summarizeTranscriptForLLM execution

**Expected Compression:**
- Small models (8K context): ~400 chars summary (~90% compression)
- Medium models (32K context): ~1000 chars summary (~70% compression)

### Test 4: YouTube Long Video (~60 min) üì∫

**Test Video:**
```bash
curl -X POST https://YOUR_LAMBDA_URL/chat \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Get transcript: https://youtube.com/watch?v=LONG_VIDEO_ID"}
    ],
    "tools": [{"name": "get_youtube_transcript"}],
    "model": "gpt-4o"
  }'
```

**Validate:**
- [ ] Full transcript available for UI
- [ ] LLM summary <= 2000 chars (for 128K context model)
- [ ] Key quotes extracted
- [ ] Compression ratio 85-95%
- [ ] Response time acceptable (< 30 seconds)

### Test 5: Monitor Token Usage üí∞

**Check CloudWatch:**
```bash
aws logs tail /aws/lambda/YOUR_FUNCTION_NAME --follow --format short | grep -i "token"
```

**Metrics to Track:**
1. **Image Selection:**
   - Baseline: 3 random images per page
   - Expected: 3 highest-value images (hero/content prioritized)
   - Improvement: Better relevance, same token count

2. **Content Truncation:**
   - Baseline: `substring(0, 300)` 
   - Expected: `extractKeyContent()` preserves more useful info in same space
   - Improvement: Better information density

3. **Transcript Compression:**
   - Baseline: Full transcript sent to LLM (10K-50K tokens)
   - Expected: Summary sent to LLM (100-500 tokens)
   - Improvement: 70-95% token reduction

**Expected Overall Savings:**
- Queries without media: No change
- Queries with images: Similar tokens, better quality
- Queries with transcripts: **70-95% token reduction**
- Average across all queries: **20-40% improvement**

---

## Rollback Plan

If issues are detected in production:

### Quick Rollback
```bash
# Revert to previous Lambda version
aws lambda update-function-code \
  --function-name YOUR_FUNCTION_NAME \
  --s3-bucket YOUR_BUCKET \
  --s3-key previous-version.zip

# Or rollback git commits
git revert HEAD
./deploy.sh
```

### Specific Feature Rollback

**Issue with image placement?**
```javascript
// In src/endpoints/chat.js, line 2405-2430
// Replace smart selection with simple:
const prioritizedImages = uniqueImages.slice(0, 3);
```

**Issue with content truncation?**
```javascript
// In src/tools.js, line 1295
// Revert to substring:
content: r.content ? r.content.substring(0, 300) : r.content
```

**Issue with transcripts?**
```javascript
// In src/tools.js, line 2925-2970
// Revert to simple return:
return JSON.stringify(result);
```

---

## Monitoring Alerts

Set up CloudWatch alarms for:

1. **Error Rate Increase**
   - Metric: Lambda errors
   - Threshold: > 5% error rate
   - Action: Send alert, investigate logs

2. **Latency Increase**
   - Metric: Lambda duration
   - Threshold: > 15 seconds (95th percentile)
   - Action: Check if transcript summarization causing delays

3. **Memory Usage**
   - Metric: Lambda max memory
   - Threshold: > 80% of allocated memory
   - Action: May need to increase memory allocation

---

## Success Criteria

### Week 1 Post-Deployment
- [ ] No errors related to new features
- [ ] Image placement working correctly (hero images prioritized)
- [ ] Content truncation preserving important info
- [ ] Transcript summarization reducing tokens

### Week 2-4 Post-Deployment
- [ ] Measure token savings (target: 20-40% on media queries)
- [ ] Collect user feedback on image quality
- [ ] Verify no regression in response quality
- [ ] Monitor cost reduction from token savings

---

## Documentation

**For Developers:**
- `IMPLEMENTATION_SUMMARY.md` - What was built and why
- `TESTING_RESULTS.md` - Test results and validation
- `WEB_SCRAPING_CONTENT_PRESERVATION_PLAN.md` - Original plan

**For Operations:**
- This file (DEPLOYMENT_GUIDE.md) - How to deploy and monitor

**Test Files:**
- `test-web-scraping.js` - Unit tests
- `test-web-scraping-integration.js` - Integration tests

---

## Support

**Issues with deployment?**
1. Check `deploy.log` for errors
2. Verify all files committed to git
3. Check Lambda function logs in CloudWatch
4. Review CloudWatch Insights for patterns

**Issues in production?**
1. Check CloudWatch logs for error messages
2. Review metrics for anomalies
3. Use rollback plan if necessary
4. Test locally with same inputs

---

## Next Steps (Phase 5 - Optional)

If you want to add UI transparency features later:

1. **Add filtering decisions tracking:**
   - Modify `extractImages()` to return removed images
   - Modify `extractLinks()` to return filtered links
   - Add reasons for each filtering decision

2. **Enhance metadata:**
   - Create `enhanceToolResultWithMetadata()` function
   - Add extraction summaries
   - Include filtering statistics

3. **UI JSON tree viewers:**
   - Display extractedContent in expandable tree
   - Show filtering decisions
   - Provide transparency into tool use

**Estimated Effort:** 12-20 hours  
**Priority:** Low (enhancement, not critical)

---

**Status:** ‚úÖ READY FOR DEPLOYMENT

Deploy with confidence - all tests passing, no syntax errors, comprehensive documentation available.

Good luck! üöÄ
