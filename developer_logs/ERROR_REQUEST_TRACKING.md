# Error Request Tracking

**Date**: October 8, 2025  
**Issue**: LLM request information not included in error responses  
**Status**: âœ… Fixed and Deployed

---

## Problem

When an error occurred during LLM API calls, the error event sent to the client only included:
```json
{
  "error": "Error message",
  "code": "ERROR"
}
```

This made debugging difficult because:
1. You couldn't see which request triggered the error
2. No visibility into the model, provider, or parameters used
3. No way to reproduce the error without full logging

**User Request**: "when there is an error, the llm info is not attached to the chat response block. i need to see the request that triggered the error"

---

## Solution

Added request tracking to the chat endpoint so that when errors occur, the complete request context is included in the error event.

### Implementation

**File**: `src/endpoints/chat.js`

#### 1. Track Last Request

Added a variable to store the most recent request before each LLM call:

```javascript
let currentMessages = [...messages];
let iterationCount = 0;
const maxIterations = parseInt(process.env.MAX_TOOL_ITERATIONS) || 20;
let lastRequestBody = null; // Track last request for error reporting

// Tool calling loop
while (iterationCount < maxIterations) {
    iterationCount++;
    
    // ... build requestBody ...
    
    // Store request body for error reporting
    lastRequestBody = {
        provider,
        model,
        request: requestBody,
        iteration: iterationCount
    };
    
    // Emit LLM request event
    sseWriter.writeEvent('llm_request', { ... });
    
    // Make streaming request
    const response = await makeStreamingRequest(targetUrl, apiKey, requestBody);
    
    // ... handle response ...
}
```

#### 2. Include Request in Error Event

Modified the error handler to include the tracked request:

```javascript
} catch (error) {
    console.error('Chat endpoint error:', error);
    
    // Build error event with request info if available
    const errorEvent = {
        error: error.message || 'Internal server error',
        code: 'ERROR',
        timestamp: new Date().toISOString()
    };
    
    // Include the last request that triggered the error
    if (lastRequestBody) {
        errorEvent.llmRequest = lastRequestBody;
        console.log('ðŸš¨ Error occurred during request:', JSON.stringify({
            provider: lastRequestBody.provider,
            model: lastRequestBody.model,
            iteration: lastRequestBody.iteration,
            messageCount: lastRequestBody.request.messages.length,
            hasTools: !!lastRequestBody.request.tools
        }, null, 2));
    }
    
    // Send error event via SSE
    if (sseWriter) {
        sseWriter.writeEvent('error', errorEvent);
    } else {
        responseStream.write(`event: error\ndata: ${JSON.stringify(errorEvent)}\n\n`);
    }
    responseStream.end();
}
```

---

## Error Response Format

### Before Fix

```json
{
  "error": "Request too large for model",
  "code": "ERROR"
}
```

### After Fix

```json
{
  "error": "Request too large for model",
  "code": "ERROR",
  "timestamp": "2025-10-08T20:29:51.234Z",
  "llmRequest": {
    "provider": "groq",
    "model": "meta-llama/llama-4-scout-17b-16e-instruct",
    "iteration": 1,
    "request": {
      "model": "meta-llama/llama-4-scout-17b-16e-instruct",
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant..."
        },
        {
          "role": "user",
          "content": "Search for climate news"
        }
      ],
      "temperature": 0.7,
      "max_tokens": 4000,
      "top_p": 1,
      "frequency_penalty": 0,
      "presence_penalty": 0,
      "tools": [
        {
          "type": "function",
          "function": {
            "name": "search_web",
            "description": "Search the web...",
            "parameters": { ... }
          }
        }
      ]
    }
  }
}
```

---

## Benefits

### 1. Complete Error Context

You can now see:
- Which provider and model was being called
- What iteration of the tool loop it was in
- The exact messages sent to the LLM
- Which tools were available
- All temperature/token parameters

### 2. Easier Debugging

When an error occurs, you can:
- Copy the exact request that failed
- Reproduce the issue locally
- Verify token counts and message structure
- Check if tools were properly formatted
- See if the error is model-specific

### 3. CloudWatch Logging

The error handler also logs structured error info:

```
ðŸš¨ Error occurred during request: {
  "provider": "groq",
  "model": "meta-llama/llama-4-scout-17b-16e-instruct",
  "iteration": 1,
  "messageCount": 5,
  "hasTools": true
}
```

This makes it easy to:
- Track error patterns by model
- Identify which iterations fail most
- Correlate errors with message count
- See if tool usage affects errors

### 4. Frontend Integration

The UI can now display the full request when showing errors:

```javascript
eventSource.addEventListener('error', (event) => {
    const errorData = JSON.parse(event.data);
    
    console.error('Error:', errorData.error);
    
    if (errorData.llmRequest) {
        console.log('Request that triggered error:');
        console.log('- Provider:', errorData.llmRequest.provider);
        console.log('- Model:', errorData.llmRequest.model);
        console.log('- Iteration:', errorData.llmRequest.iteration);
        console.log('- Messages:', errorData.llmRequest.request.messages.length);
        console.log('- Full request:', errorData.llmRequest.request);
        
        // Display in UI for user debugging
        displayErrorWithRequest(errorData);
    }
});
```

---

## Example Error Scenarios

### 1. Token Limit Exceeded

**Error Message**: `Request too large for model meta-llama/llama-4-scout-17b-16e-instruct`

**Request Context**:
```json
{
  "provider": "groq",
  "model": "meta-llama/llama-4-scout-17b-16e-instruct",
  "iteration": 3,
  "request": {
    "messages": [
      { "role": "system", "content": "..." },
      { "role": "user", "content": "Search for X" },
      { "role": "assistant", "content": "...", "tool_calls": [...] },
      { "role": "tool", "tool_call_id": "...", "content": "..." },
      { "role": "assistant", "content": "..." }
    ]
  }
}
```

**What You Learn**:
- Error occurred on iteration 3 (after 2 tool calls)
- Message history accumulated 5 messages
- Can calculate exact token count from messages
- Can test with smaller message limit

### 2. API Rate Limit

**Error Message**: `Rate limit exceeded`

**Request Context**:
```json
{
  "provider": "openai",
  "model": "gpt-4o",
  "iteration": 1,
  "request": {
    "messages": [...],
    "temperature": 0.7,
    "max_tokens": 4000
  }
}
```

**What You Learn**:
- Happened on first iteration (not accumulated)
- Can see token allocation (max_tokens: 4000)
- Can implement retry logic specific to this provider
- Can switch to different model/provider

### 3. Tool Execution Error

**Error Message**: `Tool execution failed: search_web`

**Request Context**:
```json
{
  "provider": "groq",
  "model": "llama-3.3-70b-versatile",
  "iteration": 2,
  "request": {
    "messages": [...],
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "search_web",
          "parameters": { "type": "object", ... }
        }
      }
    ]
  }
}
```

**What You Learn**:
- Error during tool calling (iteration 2)
- Can see exact tool schema that was sent
- Can verify tool definition is correct
- Can check if model supports tool calling

---

## Monitoring

### CloudWatch Queries

**Find all errors with request context**:
```
fields @timestamp, @message
| filter @message like /ðŸš¨ Error occurred during request/
| sort @timestamp desc
| limit 100
```

**Error rate by model**:
```
fields @timestamp, @message
| filter @message like /ðŸš¨ Error occurred during request/
| parse @message /"model":"(?<model>[^"]+)"/
| stats count() as error_count by model
| sort error_count desc
```

**Error rate by iteration**:
```
fields @timestamp, @message
| filter @message like /ðŸš¨ Error occurred during request/
| parse @message /"iteration":(?<iteration>\d+)/
| stats count() as error_count by iteration
| sort iteration
```

**Errors with tool usage**:
```
fields @timestamp, @message
| filter @message like /ðŸš¨ Error occurred during request/
| parse @message /"hasTools":(?<has_tools>true|false)/
| stats count() as error_count by has_tools
```

---

## Testing

### Manual Test

To verify the fix works:

1. **Trigger a token limit error**:
   ```bash
   curl -X POST https://your-lambda-url/chat \
     -H "Authorization: Bearer YOUR_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{
       "messages": [
         {"role": "user", "content": "Search for 20 different topics"}
       ],
       "model": "meta-llama/llama-4-scout-17b-16e-instruct",
       "stream": true
     }'
   ```

2. **Check the error event**:
   - Should include `llmRequest` field
   - Should show provider, model, iteration
   - Should include full request body

3. **Verify CloudWatch logs**:
   - Should see `ðŸš¨ Error occurred during request:` log
   - Should show structured error context

### Automated Test

Add to `tests/integration/error-handling.test.js`:

```javascript
describe('Error Request Tracking', () => {
  it('should include request info in error events', async () => {
    const events = [];
    
    // Make request that will fail
    const response = await makeStreamingRequest({
      messages: [{ role: 'user', content: 'Very large query...' }],
      model: 'meta-llama/llama-4-scout-17b-16e-instruct',
      stream: true
    });
    
    // Parse SSE events
    response.on('data', (chunk) => {
      const event = parseSSE(chunk);
      if (event) events.push(event);
    });
    
    await new Promise((resolve) => response.on('end', resolve));
    
    // Find error event
    const errorEvent = events.find(e => e.event === 'error');
    
    expect(errorEvent).toBeDefined();
    expect(errorEvent.data.error).toBeDefined();
    expect(errorEvent.data.llmRequest).toBeDefined();
    expect(errorEvent.data.llmRequest.provider).toBe('groq');
    expect(errorEvent.data.llmRequest.model).toBe('meta-llama/llama-4-scout-17b-16e-instruct');
    expect(errorEvent.data.llmRequest.request).toBeDefined();
    expect(errorEvent.data.llmRequest.request.messages).toBeDefined();
  });
});
```

---

## Deployment

**Date**: October 8, 2025 20:29:51 UTC  
**Method**: `make fast`  
**Package**: `llmproxy-20251008-202951.zip` (105K)  
**Status**: âœ… Deployed and Active

### Files Modified

- `src/endpoints/chat.js`
  - Added `lastRequestBody` tracking variable
  - Updated error handler to include request context
  - Added structured error logging

### Backward Compatibility

âœ… **Fully backward compatible**

- Error events still include `error` and `code` fields
- New `llmRequest` field is additive
- Existing clients can ignore the new field
- No breaking changes to error structure

---

## Summary

**Problem**: LLM request information not available when errors occur  
**Solution**: Track last request and include in error events  
**Result**: Complete error context for debugging  
**Status**: âœ… Deployed and working

### Key Improvements

- âœ… Full request context in error events
- âœ… CloudWatch logging with structured error info
- âœ… Easy error reproduction
- âœ… Model-specific error patterns visible
- âœ… Tool-related errors identifiable
- âœ… Iteration tracking for multi-turn failures
- âœ… Backward compatible with existing clients

The fix ensures that every error includes the complete request that triggered it, making debugging and error analysis much more effective.

---

**Last Updated**: October 8, 2025  
**Deployed**: October 8, 2025 20:29:51 UTC  
**Author**: GitHub Copilot
