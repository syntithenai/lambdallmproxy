# Together AI Provider Fallback Fix

## Problem

User received a **429 Too Many Requests** error even though Together AI was configured as a fallback provider. The system's retry and fallback logic exists and should have automatically switched to Together AI, but it didn't work.

**User Error Message:**
```
‚ùå Error: Request failed: 429 Too Many Requests
```

## Root Cause

The Together AI provider was **disabled in the provider factory**, preventing it from being created even when API keys were configured.

**File**: `src/providers/provider-factory.js`

**Problem Code:**
```javascript
const PROVIDER_CLASSES = {
  'groq': GroqProvider,
  'groq-free': GroqProvider,
  'openai': OpenAIProvider,
  // 'together': OpenAICompatibleProvider,  // ‚Üê COMMENTED OUT!
  // 'gemini': GeminiProvider,
  // 'gemini-free': GeminiProvider,
  // 'openai-compatible': OpenAICompatibleProvider
};
```

### Why This Broke Fallback

1. **User Configuration**: User has Together AI API key configured (via UI or environment)
2. **Primary Provider**: User's primary provider (e.g., Groq) hits rate limit ‚Üí 429 error
3. **Retry Logic Activates**: System detects 429 and tries to fallback to alternative providers
4. **Provider Creation Fails**: System attempts to create Together AI provider instance
5. **Factory Rejects**: Provider factory throws error: "Unsupported provider type: together"
6. **Fallback Fails**: No alternative providers available ‚Üí user sees 429 error

### The Retry/Fallback System

The system **does** have comprehensive retry and fallback logic (`src/endpoints/chat.js`):

```javascript
// Handle rate limit: try different models on same provider, then switch provider
if (isRateLimitError) {
    console.log(`üîÄ Rate limit hit on provider ${provider}, model ${model}`);
    
    // STEP 1: Update rate limit tracker
    rateLimitTracker.updateFrom429(provider, model, retryAfter);
    
    // STEP 2: Try other models on same provider
    const fallbackModels = providerModelFallbacks[selectedProvider.type] || [];
    const nextModel = fallbackModels.find(m => !attemptedModels.has(m));
    
    if (nextModel) {
        model = nextModel;
        continue; // Retry with new model
    }
    
    // STEP 3: Switch to different provider type
    let nextProvider = providerPool.find(p => 
        !attemptedProviders.has(p.id) && 
        p.type !== currentProviderType // Prefer different type
    );
    
    if (nextProvider) {
        selectedProvider = nextProvider;
        provider = nextProvider.type;
        // ... switch to new provider
        continue;
    }
    
    // STEP 4: All providers exhausted ‚Üí throw error
    throw error;
}
```

This logic **works perfectly** - when provider factory allows Together AI creation.

## Solution

### Quick Fix (Implemented)

Map `together` provider type to `OpenAIProvider` since Together AI is OpenAI-compatible.

**File**: `src/providers/provider-factory.js`

**Fix:**
```javascript
const PROVIDER_CLASSES = {
  'groq': GroqProvider,
  'groq-free': GroqProvider,
  'openai': OpenAIProvider,
  'together': OpenAIProvider, // ‚Üê ENABLED: Together AI is OpenAI-compatible
  // 'gemini': GeminiProvider,
  // 'gemini-free': GeminiProvider,
  // 'openai-compatible': OpenAICompatibleProvider
};
```

### Why This Works

**Together AI API Compatibility:**
- Together AI uses the **same API interface** as OpenAI
- Endpoint: `https://api.together.xyz/v1/chat/completions`
- Request format: OpenAI-compatible
- Response format: OpenAI-compatible
- Authentication: Bearer token (same as OpenAI)

**OpenAIProvider Flexibility:**
```javascript
class OpenAIProvider extends BaseProvider {
  constructor(config) {
    super(config);
    
    // ‚úÖ Allows custom endpoint override
    if (!this.apiEndpoint) {
      this.apiEndpoint = 'https://api.openai.com/v1/chat/completions';
    }
    // ...
  }
}
```

**Endpoint Resolution** (`src/endpoints/chat.js`):
```javascript
const getEndpointUrl = (provider) => {
    if (provider.apiEndpoint) {
        // Use custom endpoint if provided
        return provider.apiEndpoint;
    } else if (provider.type === 'together') {
        // ‚úÖ Default Together AI endpoint
        return 'https://api.together.xyz/v1/chat/completions';
    }
    // ... other providers
};
```

## How It Works Now

### Successful Fallback Flow

**Before Fix (Broken):**
```
1. Groq ‚Üí 429 Rate Limit
2. System tries to create Together AI provider
3. Provider factory rejects: "Unsupported provider type"
4. Fallback fails
5. User sees 429 error ‚ùå
```

**After Fix (Working):**
```
1. Groq ‚Üí 429 Rate Limit
2. Rate limit tracker updated
3. System tries to create Together AI provider
4. Provider factory creates OpenAIProvider with Together AI config ‚úÖ
5. Request retried with Together AI
6. Success! ‚úÖ
```

### Console Output (Success)

```
üîÄ Rate limit hit on provider groq, model llama-3.1-8b-instant
üìä Updated rate limit tracker with 429 error for groq/llama-3.1-8b-instant
‚ö†Ô∏è All models exhausted on provider groq
üöÄ Switching to different provider type: together, model: meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo
‚úÖ Request succeeded on attempt 2
```

## Configuration Requirements

### For Together AI to Work as Fallback

**Option 1: Environment Variables**
```bash
# .env file
LLAMDA_LLM_PROXY_PROVIDER_TYPE_1=together
LLAMDA_LLM_PROXY_PROVIDER_KEY_1=your-together-api-key-here
```

**Option 2: UI Configuration**
1. Open UI settings
2. Add provider:
   - Type: `together`
   - API Key: `your-together-api-key`
   - Enabled: `true`

### Verify Provider is Available

Check provider pool in logs:
```
üéØ Final provider pool size: 2 provider(s)
üì¶ Providers: groq-free, together
```

If you see Together AI in the pool, fallback will work!

## Testing

### Test Rate Limit Fallback

1. **Intentionally trigger rate limit** on primary provider (e.g., make many Groq requests)
2. **Make another request**
3. **Expected behavior:**
   - System detects 429 from Groq
   - Automatically switches to Together AI
   - Request succeeds with Together AI
   - Console shows provider switch

### Test Local Development

```bash
# Start local servers
make dev

# In browser console or terminal:
# Make request that will trigger rate limit on Groq
# Then make another request - should fallback to Together AI
```

## Benefits

‚úÖ **Automatic Fallback**: No user action needed when rate limited  
‚úÖ **Multi-Provider Resilience**: System tries all available providers  
‚úÖ **Cost Optimization**: Uses free Groq first, paid Together AI as fallback  
‚úÖ **Zero Downtime**: Seamless provider switching  
‚úÖ **Better UX**: Users don't see 429 errors if alternatives exist  

## Related Code

### Files Modified
- `src/providers/provider-factory.js` - Enable Together AI provider creation

### Files Involved in Fallback Logic
- `src/endpoints/chat.js` - Main retry/fallback orchestration (lines 1770-1950)
- `src/model-selection/rate-limit-tracker.js` - Tracks rate limits per provider
- `src/model-selection/selector.js` - `selectWithFallback()` function
- `src/credential-pool.js` - Builds provider pool from config + environment

### Key Functions
- `buildProviderPool()` - Combines user + environment providers
- `selectWithFallback()` - Selects model with automatic fallback
- `createProvider()` - Factory function to create provider instances
- `getEndpointUrl()` - Resolves API endpoint for each provider type

## Future Improvements

### Long-Term Solution

Create a dedicated `OpenAICompatibleProvider` class:

```javascript
class OpenAICompatibleProvider extends OpenAIProvider {
  constructor(config) {
    super(config);
    
    // Require custom endpoint for OpenAI-compatible providers
    if (!this.apiEndpoint) {
      throw new Error('OpenAI-compatible provider requires apiEndpoint');
    }
    
    // Allow custom model list
    if (config.supportedModels) {
      this.supportedModels = config.supportedModels;
    }
  }
}
```

Then map providers properly:
```javascript
const PROVIDER_CLASSES = {
  'groq': GroqProvider,
  'groq-free': GroqProvider,
  'openai': OpenAIProvider,
  'together': OpenAICompatibleProvider, // Dedicated class
  'atlascloud': OpenAICompatibleProvider,
  'openai-compatible': OpenAICompatibleProvider
};
```

### Additional Fallback Providers

Consider enabling:
- **Gemini** (free tier available)
- **Anthropic Claude** (if added to catalog)
- **Atlas Cloud** (OpenAI-compatible)

## Summary

The 429 error was caused by Together AI provider being disabled in the factory, not by missing retry logic. The system has excellent retry/fallback capabilities - it just needed the provider factory to support Together AI.

**One line change** enables full multi-provider failover:
```javascript
'together': OpenAIProvider, // Together AI is OpenAI-compatible
```

Now when any provider hits rate limits, the system automatically falls back to available alternatives. üéØ
