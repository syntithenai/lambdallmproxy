# Local Transcription - HTTP Endpoint Approach

## Problem with Previous Approach

The initial implementation used **filesystem detection** which had issues:

1. âŒ **Different code paths**: Local used `fs.readFile()`, production used `fetch()`
2. âŒ **Not realistic**: Bypassed HTTP layer, network stack, headers, CORS
3. âŒ **Hidden bugs**: Issues that would appear in production weren't caught locally
4. âŒ **LLM confusion**: LLM refused to call tool thinking localhost wouldn't work

## Better Approach: HTTP Endpoint

**Use the same HTTP flow for both local and production testing.**

### What We Have

âœ… Express static middleware already configured in `scripts/run-local-lambda.js`:
```javascript
app.use('/samples', express.static(samplesPath));
```

âœ… Sample file exists at: `ui-new/public/samples/long-form-ai-speech.mp3`

âœ… HTTP endpoint available: `http://localhost:3000/samples/long-form-ai-speech.mp3`

### What We Removed

âŒ Removed local file detection from `src/tools/transcribe.js`
âŒ No more special localhost handling
âŒ `downloadFromLocalFile()` function no longer used

### How It Works Now

**Both local and production use the same code path:**

```
User Request
    â†“
LLM calls transcribe_url tool
    â†“
Backend: downloadMedia(url)
    â†“
fetch(url)  â† Same for local AND production
    â†“
HTTP GET request
    â†“
Response with audio data
    â†“
Send to Whisper API
```

**Local**: `fetch('http://localhost:3000/samples/file.mp3')` â†’ Express static middleware  
**Production**: `fetch('https://s3.amazonaws.com/bucket/file.mp3')` â†’ S3

### Benefits

1. âœ… **Same code path** - No special cases, same behavior everywhere
2. âœ… **Realistic testing** - Tests actual HTTP requests/responses  
3. âœ… **Catches issues** - Network problems, headers, timeouts visible locally
4. âœ… **Simpler code** - No localhost detection needed
5. âœ… **LLM works** - Standard HTTP URLs, no confusion

### Testing

**Test 1: Verify HTTP endpoint serves file**
```bash
curl -I http://localhost:3000/samples/long-form-ai-speech.mp3
```

Expected output:
```
HTTP/1.1 200 OK
Content-Type: audio/mpeg
Content-Length: 2822960
```

**Test 2: Download complete file**
```bash
curl http://localhost:3000/samples/long-form-ai-speech.mp3 --output /tmp/test.mp3
ls -lh /tmp/test.mp3
```

Expected: ~2.7MB file

**Test 3: Use test script**
```bash
node test-http-endpoint.js
```

Expected: Download confirmation with file size

**Test 4: UI transcription**
1. Start servers: `make dev`
2. Open UI: http://localhost:8081
3. Click: `ğŸ  Local Dev: AI & ML discussion (~4min)`
4. Watch terminal for logs

Expected terminal logs:
```
[2025-10-18T...] GET /samples/long-form-ai-speech.mp3
ğŸŒ Fetching media from: http://localhost:3000/samples/long-form-ai-speech.mp3
ğŸ¤ Using openai Whisper API with model: whisper-1
```

### Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Local Development                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  UI (localhost:8081)                                     â”‚
â”‚       â†“                                                  â”‚
â”‚  "Transcribe this: http://localhost:3000/samples/..."   â”‚
â”‚       â†“                                                  â”‚
â”‚  POST /chat â†’ Lambda Handler                            â”‚
â”‚       â†“                                                  â”‚
â”‚  transcribe.js: fetch(localhost:3000/samples/...)       â”‚
â”‚       â†“                                                  â”‚
â”‚  GET /samples/file.mp3 â†’ Express static middleware      â”‚
â”‚       â†“                                                  â”‚
â”‚  Serves from: ui-new/public/samples/                    â”‚
â”‚       â†“                                                  â”‚
â”‚  Audio buffer â†’ Whisper API â†’ Transcript                â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Production (AWS Lambda)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  UI (llmproxy.com)                                       â”‚
â”‚       â†“                                                  â”‚
â”‚  "Transcribe this: https://s3.amazonaws.com/..."        â”‚
â”‚       â†“                                                  â”‚
â”‚  POST /chat â†’ Lambda Handler                            â”‚
â”‚       â†“                                                  â”‚
â”‚  transcribe.js: fetch(https://s3.amazonaws.com/...)     â”‚
â”‚       â†“                                                  â”‚
â”‚  S3 GET request                                          â”‚
â”‚       â†“                                                  â”‚
â”‚  Audio buffer â†’ Whisper API â†’ Transcript                â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Point

**The transcription code doesn't know or care whether it's fetching from localhost or S3**. 
It just does `fetch(url)` and processes the response. This is the correct design.

### Files Modified

1. **`src/tools/transcribe.js`** - Removed localhost detection
2. **`scripts/run-local-lambda.js`** - Express static middleware (already there)
3. **`ui-new/src/components/ChatTab.tsx`** - Updated tool description

### Next Steps

1. âœ… Servers running with `make dev`
2. âœ… HTTP endpoint configured and serving files
3. âœ… Transcription code uses standard fetch()
4. âºï¸ Test in UI with localhost URL
5. âºï¸ Verify terminal shows HTTP GET request

The implementation is now complete and uses the **realistic HTTP approach** you suggested!
